{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a457cb4-9abc-44e0-85b8-61fc839b13d5",
   "metadata": {},
   "source": [
    "# Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb4495-6493-4f9e-9819-fa04cec1c317",
   "metadata": {},
   "source": [
    "A1. \n",
    "\n",
    "Ridge regression, also known as L2 regularization, is a variant of linear regression used in machine learning and statistics. It is designed to address some of the limitations of ordinary least squares (OLS) regression, particularly when dealing with multicollinearity and overfitting.\n",
    "\n",
    "Here's an explanation of Ridge regression and how it differs from ordinary least squares regression:\n",
    "\n",
    "Ridge Regression:\n",
    "\n",
    "1. Objective Function: Ridge regression modifies the OLS objective function by adding a penalty term based on the sum of the squared coefficients (parameters). The objective function in Ridge regression is to minimize the sum of squared residuals (errors) between the model's predictions and the actual observed values, while also minimizing the sum of the squared coefficients:\n",
    "\n",
    "- Equation \n",
    "\n",
    "    - The first part of the equation represents the standard OLS loss, aiming to minimize the fit to the training data.\n",
    "    - The second part is the Ridge penalty term, where λ is the regularization parameter that controls the strength of the regularization.\n",
    "\n",
    "2. Effect on Coefficients: Ridge regularization shrinks the coefficients of the model toward zero without setting them exactly to zero. This means that all predictor variables (features) remain in the model, but their coefficients are reduced in magnitude.\n",
    "\n",
    "3. Purpose: The primary purpose of Ridge regression is to prevent overfitting, especially when dealing with high-dimensional data or datasets with multicollinearity (correlation among predictor variables). It helps stabilize the model by reducing the variance in the coefficient estimates.\n",
    "\n",
    "Differences from Ordinary Least Squares (OLS) Regression:\n",
    "\n",
    "- Regularization: The key difference is that Ridge regression introduces L2 regularization by adding the penalty term based on the sum of squared coefficients. OLS does not include any regularization; it aims to minimize the sum of squared residuals only.\n",
    "- Coefficients: In OLS regression, the model's coefficients are estimated without any constraints, and they can take any value. In Ridge regression, the coefficients are shrunk toward zero, but none of them are set exactly to zero.\n",
    "- Handling Multicollinearity: Ridge regression is particularly useful when dealing with multicollinearity, where predictor variables are highly correlated. It helps prevent the instability and erratic behavior of coefficient estimates that can occur in OLS when multicollinearity is present.\n",
    "- Bias-Variance Trade-off: Ridge regression introduces a bias into the model by shrinking coefficients, but it reduces the model's variance, leading to better generalization. OLS has no such bias-variance trade-off.\n",
    "\n",
    "In summary, Ridge regression is a modification of ordinary least squares regression that introduces L2 regularization to prevent overfitting and stabilize coefficient estimates. It is especially useful in scenarios where multicollinearity or a large number of predictors are present, as it helps produce a more robust and generalizable model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bd9f3b-7009-4b41-a817-f5489675a6e9",
   "metadata": {},
   "source": [
    "# Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9759b42a-d84a-4bff-af8d-b6d3d3f00756",
   "metadata": {},
   "source": [
    "A2.\n",
    "\n",
    "Ridge regression shares many of the same assumptions with ordinary least squares (OLS) regression, as it is essentially a modified form of linear regression. These assumptions are important to ensure the validity and reliability of the ridge regression model. The key assumptions of ridge regression are as follows:\n",
    "\n",
    "1. Linearity: The relationship between the predictor variables (features) and the target variable is assumed to be linear. This means that changes in the predictor variables are associated with proportional changes in the target variable.\n",
    "\n",
    "2. Independence of Errors: The errors (residuals) in the model should be independent of each other. In other words, the error for one observation should not provide any information about the error for another observation.\n",
    "\n",
    "3. Homoscedasticity: The variance of the errors should be constant across all levels of the predictor variables. This means that the spread of residuals should be roughly the same for all values of the predictors.\n",
    "\n",
    "4. Multicollinearity: Ridge regression assumes that the predictor variables are not perfectly multicollinear. Multicollinearity occurs when two or more predictor variables are highly correlated, making it difficult to distinguish their individual effects on the target variable.\n",
    "\n",
    "5. No Perfect Collinearity: Perfect collinearity, where one predictor variable can be expressed as a linear combination of others, should be avoided. Perfect collinearity leads to non-identifiability of coefficients and makes it impossible to estimate unique parameter values.\n",
    "\n",
    "6. Mean-Centered Predictors: While not a strict assumption, it is often recommended to center the predictor variables by subtracting their means. Centering helps in the interpretation of the intercept term and can improve numerical stability in ridge regression.\n",
    "\n",
    "7. Normality of Residuals: Ridge regression does not assume that the residuals follow a normal distribution, as ordinary least squares does. However, if you plan to make statistical inferences or hypothesis tests based on the residuals, normality might be relevant.\n",
    "\n",
    "It's important to note that ridge regression is more robust to violations of the assumption of multicollinearity compared to ordinary least squares regression. In fact, one of its primary purposes is to address multicollinearity. However, it does introduce the additional assumption of L2 regularization, which is not present in OLS regression.\n",
    "\n",
    "While ridge regression can be more forgiving of certain violations of assumptions, it's still essential to assess the appropriateness of the assumptions for your specific data and modeling goals. Diagnostic tools and techniques, such as residual analysis and multicollinearity checks, can help assess the validity of these assumptions in the context of ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15da9238-a131-445e-891c-ea28e0caf0f1",
   "metadata": {},
   "source": [
    "# Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ccf248-fbc2-45a2-972b-524bf7ae3f91",
   "metadata": {},
   "source": [
    "A3\n",
    "\n",
    "The tuning parameter λ in Ridge Regression controls the strength of the regularization. Selecting an appropriate value for  λ is a crucial step in building an effective Ridge regression model. The goal is to strike a balance between model simplicity (higher λ) and model fit to the training data (lower λ). Here are some common methods for selecting the value of λ in Ridge Regression:\n",
    "\n",
    "1. Cross-Validation:\n",
    "- Cross-validation is one of the most widely used methods for selecting the value of λ. The idea is to train and evaluate the Ridge regression model on different subsets of the training data while varying λ. Common cross-validation techniques include k-fold cross-validation and leave-one-out cross-validation (LOOCV).\n",
    "    - k-fold Cross-Validation: Split the training data into k subsets (folds). Train the Ridge regression model on k−1 folds and validate it on the remaining fold. Repeat this process k times, each time using a different fold as the validation set. Calculate the average performance metric (e.g., mean squared error) across all iterations for each λ. Choose the λ that results in the best performance.\n",
    "    - LOOCV: Similar to k-fold cross-validation, but with k equal to the number of observations in the training data. This approach is computationally expensive but provides a more reliable estimate of model performance.\n",
    "\n",
    "2. Grid Search:\n",
    "- Perform a grid search over a predefined range of λ values. Train Ridge regression models for each λ in the grid and evaluate their performance using a validation set or cross-validation. Choose the λ that yields the best performance.\n",
    "\n",
    "3. Regularization Path Algorithms:\n",
    "- Some specialized algorithms, such as coordinate descent or gradient descent, can efficiently compute the entire regularization path (a sequence of λ values) and corresponding coefficient estimates. This allows you to visualize how the coefficients change as λ varies, helping you choose an appropriate value based on the desired level of regularization.\n",
    "\n",
    "4. Information Criteria:\n",
    "- Criteria like Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC) can be used to select λ. These criteria balance model fit and complexity. A lower AIC or BIC indicates a better trade-off between model fit and regularization.\n",
    "\n",
    "5. Validation Set:\n",
    "- Split your training data into a training set and a separate validation set. Train Ridge regression models with different λ values on the training set and evaluate their performance on the validation set. Choose the λ that performs best on the validation set.\n",
    "\n",
    "6. Domain Knowledge:\n",
    "- In some cases, domain knowledge or prior information about the problem can guide the choice of λ. For example, if you have reason to believe that a certain level of regularization is appropriate, you can start with that value and refine it using cross-validation or other methods.\n",
    "\n",
    "The choice of the method for selecting λ depends on factors such as the size of your dataset, available computational resources, and the specific goals of your analysis. Cross-validation is generally a robust and widely applicable approach. It provides an unbiased estimate of model performance and helps prevent overfitting by selecting an appropriate λ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d7d3f7-abc4-4a77-ad6c-9c895b6ab1b3",
   "metadata": {},
   "source": [
    "# Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aaf6c7-5b46-48e9-8de2-95a5e2521894",
   "metadata": {},
   "source": [
    "A4.\n",
    "\n",
    "Yes, Ridge Regression can be used for feature selection, although it's important to note that its primary purpose is not feature selection but rather regularization to prevent overfitting. However, Ridge Regression does have the side effect of shrinking the coefficients of less important features toward zero without setting them exactly to zero. This feature reduction can effectively serve as a form of feature selection. Here's how Ridge Regression can be used for feature selection:\n",
    "\n",
    "1. Shrinking Coefficients:\n",
    "- Ridge Regression introduces a penalty term in the objective function that encourages the model to minimize the sum of squared coefficients. As λ, the regularization parameter, increases, the magnitude of the coefficients decreases. Coefficients of less important features are more likely to be pushed closer to zero, while coefficients of important features are retained to some extent.\n",
    "\n",
    "2. Feature Ranking:\n",
    "- By examining the magnitude of the coefficients estimated by Ridge Regression for different values of λ, you can rank the features in terms of their importance. Features with larger coefficients, even after regularization, are considered more important in predicting the target variable.\n",
    "\n",
    "3. Thresholding:\n",
    "- You can set a threshold value, below which coefficients are considered negligible and effectively set to zero. This threshold can be determined based on your problem's requirements or through cross-validation. Features with coefficients below the threshold are essentially removed from the model.\n",
    "\n",
    "4. Selecting the Optimal λ:\n",
    "- To effectively perform feature selection, you'll need to select an appropriate value of λ. This is typically done using cross-validation or other methods for hyperparameter tuning. Cross-validation helps you find the λ value that balances model fit and feature selection.\n",
    "\n",
    "5. Iterative Process:\n",
    "- Feature selection with Ridge Regression can be an iterative process. You start with a wide set of features, train a Ridge Regression model with various λ values, observe the effect on feature coefficients, and gradually refine the set of features by adjusting the threshold or the range of λ values.\n",
    "\n",
    "6. Trade-off:\n",
    "- Keep in mind that Ridge Regression does not perform feature selection as aggressively as Lasso Regression, which can set coefficients exactly to zero. Ridge retains all features to some extent, which may not be desirable if you have strong evidence that certain features are irrelevant. If you require more aggressive feature selection, you might consider Lasso Regression instead.\n",
    "\n",
    "In summary, while Ridge Regression primarily focuses on regularization to prevent overfitting, it can be used for feature selection by leveraging its ability to shrink coefficients. By adjusting the regularization parameter λ and applying thresholding, you can effectively identify and retain the most important features while discarding less relevant ones. However, if you prioritize aggressive feature selection, Lasso Regression might be a more suitable choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f7cb3e-a825-4e3d-9d14-a00d372e82f0",
   "metadata": {},
   "source": [
    "# Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34acae7-d8f6-460b-b996-dd9e1b99f489",
   "metadata": {},
   "source": [
    "A5.\n",
    "\n",
    "Ridge Regression is particularly well-suited to handle multicollinearity, a situation where predictor variables (features) in a regression model are highly correlated with each other. Multicollinearity can pose problems in ordinary least squares (OLS) regression, but Ridge Regression can mitigate its effects effectively. Here's how Ridge Regression performs in the presence of multicollinearity:\n",
    "\n",
    "1. Reduction of Coefficient Variance: One of the primary benefits of Ridge Regression is that it reduces the variance of the coefficient estimates. In the presence of multicollinearity, the coefficient estimates in OLS can be highly unstable and sensitive to small changes in the data. Ridge Regression stabilizes these estimates by shrinking the coefficients toward zero. This helps prevent erratic behavior of coefficient estimates due to multicollinearity.\n",
    "\n",
    "2. Balancing Act: Ridge Regression strikes a balance between model fit to the training data and model simplicity. It does this by adding a penalty term to the objective function that discourages large coefficient values. In the presence of multicollinearity, some coefficients that would have been excessively large in OLS are moderated by Ridge Regression, making the model more robust.\n",
    "\n",
    "3. Feature Retention: Unlike Lasso Regression, which can perform aggressive feature selection and set some coefficients to exactly zero, Ridge Regression retains all features to some extent. While it shrinks coefficients, it does not eliminate them entirely. This can be advantageous when multicollinearity is present, as it avoids the removal of potentially relevant features.\n",
    "\n",
    "4. Partial Correlation: Ridge Regression effectively estimates the partial correlation between each predictor and the target variable, accounting for the presence of other predictors. This means that even in the presence of multicollinearity, Ridge Regression can provide valuable information about the strength and direction of relationships between individual predictors and the target.\n",
    "\n",
    "5. Regularization Strength: The extent to which Ridge Regression mitigates multicollinearity depends on the choice of the regularization parameter λ. Larger values of λ result in stronger regularization, which can be more effective at reducing multicollinearity but may lead to a simpler model with smaller coefficients.\n",
    "\n",
    "6. Interpretation: While Ridge Regression stabilizes coefficient estimates and can improve the reliability of the model in the presence of multicollinearity, it may make the interpretation of coefficients less straightforward. The coefficients no longer represent the marginal effect of a one-unit change in a predictor while holding all other predictors constant.\n",
    "\n",
    "In summary, Ridge Regression is a valuable tool for dealing with multicollinearity in regression analysis. It helps stabilize coefficient estimates, prevents overfitting, and retains all features to some extent. By selecting an appropriate value for the regularization parameter λ, you can control the extent to which multicollinearity is addressed while maintaining model performance. However, it's important to note that Ridge Regression does not provide as aggressive feature selection as Lasso Regression, which sets some coefficients to exactly zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7c5d9d-0efa-45fc-9e1c-dd0abb788b66",
   "metadata": {},
   "source": [
    "# Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d8ad4e-60a1-4260-a87f-efe49aea98e7",
   "metadata": {},
   "source": [
    "A6\n",
    "\n",
    "Yes, Ridge Regression can handle both categorical and continuous independent variables (predictors) as part of the feature set in a regression model. However, some considerations and preprocessing steps are necessary when dealing with categorical variables in Ridge Regression:\n",
    "\n",
    "1. Encoding Categorical Variables:\n",
    "- Categorical variables need to be encoded into a numerical format because Ridge Regression, like other linear regression techniques, works with numerical inputs. There are several ways to encode categorical variables:\n",
    "    - One-Hot Encoding: This is a common technique where each category of a categorical variable is transformed into a binary (0 or 1) column. Each column represents one category, and a 1 is placed in the column corresponding to the category present in the original data. This method is useful when there is no inherent order among categories.\n",
    "    - Label Encoding: This method assigns a unique integer to each category. This encoding can be appropriate for ordinal categorical variables where there is a natural order among the categories.\n",
    "    - Binary Encoding, Ordinal Encoding, etc.: Depending on the nature of your categorical variables, other encoding methods may be suitable.\n",
    "\n",
    "2. Dummy Variables: When using one-hot encoding, be aware of the \"dummy variable trap,\" which occurs when one of the one-hot encoded variables can be predicted from the others due to perfect multicollinearity. To avoid this, you typically drop one of the columns, resulting in n−1 binary columns for a categorical variable with n categories.\n",
    "\n",
    "3. Scaling: Ridge Regression is sensitive to the scale of the variables, so it's important to standardize or normalize all variables, including both continuous and encoded categorical variables. This ensures that the regularization term treats all variables equally. Common scaling methods include z-score standardization or min-max scaling.\n",
    "\n",
    "4. Regularization Strength: When using Ridge Regression with a mix of continuous and categorical variables, you'll need to select an appropriate value for the regularization parameter λ. The choice of λ depends on the specific characteristics of your data and modeling goals.\n",
    "\n",
    "5. Interpretation: Keep in mind that interpreting the coefficients of categorical variables in Ridge Regression is not always straightforward. The coefficients represent the change in the target variable associated with a one-unit change in the predictor, which may not have a clear interpretation for categorical variables, especially when using one-hot encoding.\n",
    "\n",
    "In summary, Ridge Regression can handle a mix of categorical and continuous independent variables, but proper preprocessing, encoding, and scaling are essential. Categorical variables need to be transformed into a numerical format, and all variables should be standardized or normalized. The choice of encoding method and regularization strength should align with the nature of your data and modeling objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8a0a14-c07a-47ea-9872-5edc66fbe310",
   "metadata": {},
   "source": [
    "# Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4619a787-a7f9-4e54-822b-16ba2f54dc65",
   "metadata": {},
   "source": [
    "A7\n",
    "\n",
    "Interpreting the coefficients of Ridge Regression is similar to interpreting the coefficients in ordinary least squares (OLS) regression, with some important considerations due to the regularization introduced by Ridge Regression. Here's how to interpret the coefficients:\n",
    "\n",
    "1. Magnitude and Sign:\n",
    "- The magnitude of the coefficient (β) indicates the strength of the relationship between the predictor variable and the target variable. A larger magnitude implies a stronger impact on the target variable.\n",
    "- The sign of the coefficient (β) indicates the direction of the relationship. A positive coefficient means that as the predictor variable increases, the target variable is expected to increase, and vice versa for a negative coefficient.\n",
    "\n",
    "2. Magnitude Relative to Other Coefficients:\n",
    "- In Ridge Regression, the coefficients are shrunk toward zero to prevent overfitting. Therefore, the magnitude of the coefficients in Ridge Regression is typically smaller than in OLS regression.\n",
    "- The relative magnitude of coefficients is still informative. You can compare the magnitudes to understand which predictors have a more substantial impact on the target variable, even if they are all shrunk to some extent.\n",
    "\n",
    "3. Feature Importance:\n",
    "- In Ridge Regression, you can interpret feature importance based on the magnitude of the coefficients. Features with larger, non-zero coefficients are relatively more important in predicting the target variable.\n",
    "- Features with coefficients that are close to zero are less influential in the model, but they are not entirely excluded.\n",
    "\n",
    "4. Intercept Term:\n",
    "- The intercept term (β0) represents the estimated target variable value when all predictor variables are zero. Its interpretation is similar to that in OLS regression.\n",
    "\n",
    "5. Caution with One-Hot Encoding:\n",
    "- If you have categorical variables encoded with one-hot encoding, the coefficients for the categories of a categorical variable should be interpreted relative to the reference category (the one that was omitted to avoid the dummy variable trap).\n",
    "- For continuous variables and ordinal categorical variables, the interpretation is more straightforward.\n",
    "\n",
    "6. Normalization/Scaling Impact:\n",
    "- The interpretation of coefficients can be influenced by the scaling or normalization of predictor variables. It's important to standardize or normalize predictor variables before running Ridge Regression so that the coefficients are on the same scale.\n",
    "\n",
    "7. Limitation: Ridge Regression, like other linear regression techniques, provides an interpretation of the relationship between predictors and the target variable under the assumption of linearity. If the true relationship in the data is nonlinear, the interpretation may not be accurate.\n",
    "\n",
    "It's crucial to remember that Ridge Regression is often used for its regularization properties to prevent overfitting rather than for coefficient interpretation. If detailed interpretation of coefficients is a primary goal, it's important to consider other regression techniques that do not involve regularization or that allow for more straightforward interpretation, such as simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9e70a0-5c96-4958-a8cc-08ebb25a3258",
   "metadata": {},
   "source": [
    "# Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c01543-457b-4647-a381-e5ae6f6d3ac9",
   "metadata": {},
   "source": [
    "A8: \n",
    "\n",
    "Ridge Regression can be adapted for time-series data analysis, but it requires some modifications and considerations to account for the temporal nature of the data. Time-series data often exhibit autocorrelation, seasonality, and trends, which are characteristics that standard Ridge Regression may not handle effectively. Here's how you can use Ridge Regression for time-series data analysis:\n",
    "\n",
    "1. Feature Engineering:\n",
    "- In time-series analysis, the choice of predictors (features) is critical. You may need to create lagged versions of the target variable or other relevant variables as features. These lagged features capture dependencies over time.\n",
    "- Additional features, such as time-related features (e.g., day of the week, month, year) or external factors that can influence the target variable, should be considered.\n",
    "\n",
    "2. Stationarity:\n",
    "- Check for stationarity in the time series. Ridge Regression assumes that the data is stationary, meaning that the statistical properties of the data do not change over time. If your data is not stationary, consider differencing or other transformations to achieve stationarity.\n",
    "\n",
    "3. Time-Based Encoding:\n",
    "- Incorporate the time dimension into your feature set. This can include time-related variables, such as the time index or cyclical encoding for seasonality (e.g., sine and cosine functions for capturing periodic patterns).\n",
    "\n",
    "4. Regularization Parameter Selection:\n",
    "- When using Ridge Regression for time-series data, you still need to select an appropriate value for the regularization parameter (λ). Cross-validation or other techniques can help you choose the optimal λ value.\n",
    "- Be cautious about over-regularizing the model, as you may risk losing important temporal patterns.\n",
    "\n",
    "5. Temporal Cross-Validation:\n",
    "- When performing cross-validation, ensure that you use time-aware cross-validation techniques, such as time series cross-validation (e.g., rolling-window or expanding-window cross-validation). This helps prevent data leakage and maintains the temporal order of observations.\n",
    "\n",
    "6. Evaluation Metrics:\n",
    "- Use appropriate evaluation metrics for time-series forecasting, such as Mean Absolute Error (MAE), Mean Squared Error (MSE), or root Mean Squared Error (RMSE). Additionally, consider time-specific metrics like Mean Absolute Percentage Error (MAPE) or forecast horizon-dependent metrics.\n",
    "\n",
    "7. Dynamic Forecasting:\n",
    "- Time-series data often involve forecasting future values. In Ridge Regression, you can use the model to make dynamic forecasts by iteratively updating the model with new observations as they become available.\n",
    "\n",
    "8. Model Assumptions:\n",
    "- Be aware that Ridge Regression, like other linear regression techniques, assumes a linear relationship between predictors and the target variable. This may not fully capture complex temporal dependencies. Consider other time-series-specific models like ARIMA, SARIMA, or state-space models for more advanced modeling of time-series data.\n",
    "\n",
    "In summary, Ridge Regression can be adapted for time-series data analysis by incorporating time-related features, addressing stationarity, and choosing appropriate evaluation metrics and cross-validation methods. However, it may not capture all aspects of time-series data, and for more complex time-series modeling, specialized techniques should be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3966fa4a-a0eb-4c07-aac4-6d2a8157c477",
   "metadata": {},
   "source": [
    "# Note : Respacted Sir, In some questions I have not write equation of perticulars. But I know the all the equations. Here I am not able to write equestion in JupyterLab. Thank you so much sir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9550a7-bceb-4a63-83e9-15ad6c2d5014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
